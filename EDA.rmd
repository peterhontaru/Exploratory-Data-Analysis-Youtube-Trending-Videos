---
output: 
  html_document: 
    highlight: pygments
    theme: readable
    title: "YouTube In-depth Trending Analysis"
    toc: yes
    toc_depth: 4
    
    fig_width: 6 
    fig_height: 6
---
# YouTube Trending Videos

# 1) Background

### Why this dataset?
YouTube has been a massive influence and source of knowledge both in my professional life as well in my personal goals/ambitions. Thus, I thought it would be interesting to explore this dataset and see what insights I could come up with

### What are Trending Videos and why are they important? 
Trending videos work alongside the home page to provide users with content to watch. 

While the Home feed is **highly** personalised on previous views, what the user watched longest, engagement, subscriptions, the trending page is very broad and is the same across all accounts.

Since it displays this across hundreds of thousands of accounts, this makes it a great source of views for content creators.

### Purpose of this analysis:
1. Develop an understanding of the YouTube Trending Videos
2. Assess differences between English-speaking countries (Canada, Great Britain, USA)
                                      
**Thus, all throughout, we will generate questions, answer them with data and then further refine these questions based on what we found out.**

### To whom might this be helpful?
*content creators / marketing agencies can get a better understanding of the audience which would help tailor content
*data science enthusiasts as they might get new ideas for how to use R (for beginners) or see other people's analyses (for intermediates)
*people that are generally interested in YouTube





## Key Insights:
* CA tends to have videos that only trend for a very short period, while in the GB/US they typically trend significantly longer (up to 38 days)





## Dataset information:
* Trending data between "2017-11-14" and "2018-06-14" for US, CA and GB
* Contains > 120,000 videos information in the tree countries


#videos by day                    #total videos




# 2) Data import, tidying, cleaning

The following packages were needed to produce the output:
``` {r results = "hide", warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(lubridate)
library(data.table)
library(readr)
library(rjson)
library(jsonlite)
library(ggcorrplot)
```

Download data:

All the data is downloaded from <https://www.kaggle.com/datasnaek/youtube-new>.

Raw data files are available within the "Datasets" folder.

If you'd like to skip the data importing, manipulating and cleaning, a csv file of cleaned and parsed data is also included under "raw_data.csv".

### 2.1. Import

``` {r results = "hide", warning = FALSE, message = FALSE}
#get datasets for the countries we're interested in
gb_data <- read_csv("~/DS/YouTube - EDA/Datasets/GBvideos.csv")
us_data <- read_csv("~/DS/YouTube - EDA/Datasets/USvideos.csv")
ca_data <- read_csv("~/DS/YouTube - EDA/Datasets/CAvideos.csv")

#add a flag so that we know which country belongs to which dataset
gb_data$country <- "GB"
us_data$country <- "US"
ca_data$country <- "CA"

#get category data
us_cat_json <- fromJSON("~/DS/YouTube - EDA/Datasets/US_category_id.json")
gb_cat_json <- fromJSON("~/DS/YouTube - EDA/Datasets/GB_category_id.json")
ca_cat_json <- fromJSON("~/DS/YouTube - EDA/Datasets/CA_category_id.json")
summary(us_cat_json)

#bind together
US_category <-  as.data.frame(cbind(us_cat_json[["items"]][["id"]], us_cat_json[["items"]][["snippet"]][["title"]]))
GB_category <-  as.data.frame(cbind(gb_cat_json[["items"]][["id"]], gb_cat_json[["items"]][["snippet"]][["title"]]))
CA_category <-  as.data.frame(cbind(ca_cat_json[["items"]][["id"]], ca_cat_json[["items"]][["snippet"]][["title"]]))

#change column names
names(US_category) <- c("category_id","category_title")
names(GB_category) <- c("category_id","category_title")
names(CA_category) <- c("category_id","category_title")

#merge data
us_data <- merge(x = us_data, y = US_category, by = "category_id")
gb_data <- merge(x = gb_data, y = GB_category, by = "category_id")
ca_data <- merge(x = ca_data, y = CA_category, by = "category_id")

#combine into one dataset and remove previous variables from memory
raw_data <- as.data.table(rbind(gb_data, us_data, ca_data))

rm(us_data, gb_data, ca_data)
rm(US_category, GB_category, CA_category)
rm(us_cat_json, gb_cat_json, ca_cat_json)

```

### 2.2. Clean and Tidy Data

```{r}
#I usually like to save a backup of the raw data to make it easier to revert changes so let's do that
raw_data_backup <- raw_data

#let's check the structure of the data
glimpse(raw_data)
#we can spot a few class problems as well as add some additional columns

#clean and format dates/times
raw_data$trending_date <- ydm(raw_data$trending_date)
raw_data$publish_date <- ymd(substr(raw_data$publish_time,start = 1,stop = 10))
raw_data$hour <- format(strptime(raw_data$publish_time,"%Y-%m-%d %H:%M:%S"),'%H')

raw_data$days_diff <- as.numeric(raw_data$trending_date-raw_data$publish_date)

#remove unnecessary data
raw_data <- raw_data %>%
        select(-description, -tags, -category_id, -publish_time)

#add new columns for further analysis
raw_data <- raw_data %>%
        mutate(perc_engagement = round((likes + dislikes + comment_count) / views, digits = 2)*100,
               perc_likes = round(likes / (likes+dislikes), digits=2)*100,
               perc_comments = round(comment_count/views, digits = 2) * 100)

#we also want to change hour and country fields to factors
raw_data$hour <- as.factor(raw_data$hour)
raw_data$country <- as.factor(raw_data$country)

#remove video_error_or_removed videos as we do not want these
table(raw_data$video_error_or_removed)

#it's a very small dataset so we will just remove without looking into these too much as they were deleted/errors/copyright violations
video_error_or_removed <- raw_data %>% 
        filter (video_error_or_removed == "TRUE")%>%
        select(country, channel_title, title, video_id) %>%
        group_by(country, channel_title, title, video_id) %>%
        summarize(count = n()) %>% #count = days_trending
        arrange(desc(count)) %>%
        print()

raw_data <- raw_data %>%
        filter (video_error_or_removed == "FALSE") %>%
        select (-video_error_or_removed)
        
rm(video_error_or_removed)

#-----------------------------------------CHECK FOR NAs
raw_data %>% summarise_all(~ sum(is.na(.)))
#is this because of ratings being disabled?
raw_data %>% filter(is.na(perc_likes)) %>% group_by(ratings_disabled) %>% summarize (count = n())
#it looks like that's the case for al but 3 of them. Those are na due to not having any likes/dislikes so we will adjust the calculation slightly
raw_data <- raw_data %>% mutate(perc_likes = ifelse(ratings_disabled == "FALSE" & likes==0, 0, round(likes / (likes+dislikes), digits=2)*100))

raw_data %>% filter(is.na(perc_likes)) %>% group_by(ratings_disabled) %>% summarize (count = n())
#this error has been fixed now and NAs are only showing for those with ratings disabled

#-----------------------------------------CHECK FOR NULLS
raw_data %>% summarise_all(~ sum(is.null(.)))
#no issues here

#back up our existing data into raw_data_backup
raw_data_backup <- raw_data
```


### 2.3. Data understanding

At this point, there are a few columns that we need to look into further as I am not fully confident I understand: 
* comments_disabled
* ratings_disabled
* hour

``` {r}

#comments_disabled
table(raw_data$comments_disabled)
table(quantile(raw_data$comments_disabled, probs = seq(0, 1, length.out=101)))
#~1% of the data has comments disabled

#ratings_disabled
table(raw_data$ratings_disabled)
table(quantile(raw_data$ratings_disabled, probs = seq(0, 1, length.out=101)))
#<1% of the data has ratings disabled

#although they're not a significant chunk of the dataset, we won't remove these from the raw data, as these are genuine videos
#it is likely that these might be more controversial and we will look at this later in the data exploration process
#the dataset is before the rule with videos aimed to children will not be allowed comments so that's not a factor
#however, they will be removed from engagement-specific analysis as non existent data (0s would act as outliers)


#-----------------------------------------COMMENTS DISABLED ANALYSIS
#1
table(raw_data$comments_disabled)

#2
#facet by country but not by category
comments_disabled_by_country <- raw_data %>%
  select (country, likes, dislikes, comments_disabled) %>%
  group_by (country, comments_disabled) %>%
  summarize(count = n(),
            dislikes_perc = round(sum(dislikes)/(sum(likes)+sum(dislikes))*100,0)) %>%
  print()

ggplot(comments_disabled_by_country, aes(country, dislikes_perc , fill = comments_disabled)) + 
  geom_col(position = "dodge")

#potentially some controversial videos choose to take comments down to avoid backlash (particularly in news and entertainment)
#these look to be more dislikes, on average than likes (but sample size is also much smaller so exceptions are likely to skew data)


#-----------------------------------------RATINGS DISABLED ANALYSIS

ratings_disabled <- raw_data %>% 
  filter (ratings_disabled == "TRUE") %>%
  select (category_title, comment_count) %>%
  group_by (category_title) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  print

#remove from memory
rm(ratings_disabled, comments_disabled_by_country)
```

# 3. Analysis

## 3.1. Correlation

Let's try and check and covariation between our main variables: 

```{r}
raw_data_corr<- raw_data %>% select(views,likes,dislikes, comment_count, days_diff)

# Compute a correlation matrix
corr <- round(cor(raw_data_corr), 2)
corr

{results = "hide"}
# Compute a matrix of correlation p-values
pmat <- cor_pmat(raw_data_corr)
pmat
```
Let's see what this looks like:

```{r}
# Visualize the correlation matrix
ggcorrplot(corr, method = "square", 
           ggtheme = ggplot2::theme_minimal, 
           title = "Correlation plot",
           outline.col = "black",
           colors = c("blue","white", "red"),
           lab = TRUE,
           digits = 2)
```


### Key insights:
* the highest correlation was between **views** and **likes **; however, as with all correlations, this does not explain which caused which
* high correlation between  **likes ** and  **comment count **, meaning that people engaged a lot on the videos they liked *but*
* there was also a high correlation between  **dislikes ** and  **comment count **, meaning people engaged in comments also on videos they disliked/controversial videos


```r
#remove variables
rm(corr, pmat, raw_data_corr)
```


## 3.2. Views
### **3.2.1. What do the overall stats look like for views?**


```{r}

#1. Let's have a look at the overall stats for the trending videos
trending_summary <- raw_data %>%
  select(country,video_id) %>%
  group_by(country) %>%
  summarize(count = n(),
            unique = n_distinct(video_id))

ggplot(trending_summary)+
  geom_col(aes(country, count, fill = country), show.legend = FALSE)+
  geom_line(aes(country,unique), group = 1, lwd = 4, col = "black", lty = 3)+
  geom_label(aes(country, unique, label = unique), fill = "black", col = "white")+
  annotate("text", x = 2.5, y = 7000, label = "Unique Trending Videos", angle = 7)+
  labs(y = "Total Trending Videos",
       x = "Country",
       title = "CA has a significantly different profile where a significant part of videos only trend for one day")
```


### **2.2. Overall, what's the spread of views (in million) for those that reached trending**


```{r}
views_data <- raw_data %>%
  select(country,video_id,views) %>%
  group_by(country, video_id) %>%
  summarize(count = n(),
            unique = n_distinct(video_id),
            views = max(views),
            views_m_reached = floor(views/1000000),
            views_rounded = round(views/1000000))

#let's look at a basic spread
summary(views_data$views_m_reached)
#dive into more detail
table(quantile(views_data$views_m_reached, probs = seq(0, 1, length.out=101)))
#76% did not make it past 1 mil
#only 5% go over the 5 million mark but not as detailed as we'd like so let's see by 1% increments
quantile(views_data$views_m_reached, probs = seq(0, 1, length.out=101))
#99% of the data is between 0 days from publish to trending and 18 days so we will use this 99th percentile as a max limit for our graphs
max_limit <- quantile(views_data$views_m_reached, probs = c(0.99))

ggplot(views_data, aes(views_m_reached, count))+
  geom_col(fill="dark red")+
  coord_cartesian(xlim=c(0,max_limit))+
  labs(y = "Total Trending Videos",
       x = "Views (millions)",
       title = "Total Trending Videos by Views reached while Trending")+
  theme_gray()+
  geom_vline(xintercept=0.5, lwd = 1, col = "black", lty = 1)+
  annotate("text", x = 0.7, y = 28000, label = "76% of data", angle = 90)+
  geom_vline(xintercept=4.5, lwd = 1, col = "black", lty = 5)+
  annotate("text", x = 4.7, y = 28000, label = "95% of data", angle = 90)+
  geom_vline(xintercept=max_limit+0.5, lwd = 1, col = "black", lty = 2)+
  annotate("text", x = max_limit-0.3+0.5, y = 28000, label = "99% of data", angle = 90)
#less than 62% of videos ever reach 1m views and <5% go over 5m
#only 1% go over 19m with the highest being 425m

ggplot(views_data)+
geom_bar(aes(views_m_reached, ..prop.., fill = country))+
facet_grid(~country)+
coord_cartesian(xlim=c(0,max_limit))+
theme_gray()+
labs(y = "% of Total Trending Videos",
       x = "Views (millions)",
       fill = "Country",
       title = "% of Trending Videos by Views reached while Trending - Segmentation by Country")
#switched to % view because there is a significant difference in volume betwee nthe countries. We can see a wider distirbution in GB and US vs CA
```


### **2.3. Because a significant part of the trending videos never reached 1m, it would be helpful to look at this subset in particular**

```{r}
views_data_below1m <- views_data %>%
  filter (views_m_reached < 1)

#Overall, what's the spread of views for those that reached trending with under 1m views
ggplot(views_data_below1m, aes(views))+
  geom_histogram(fill="dark red", binwidth = 10000)+
  labs(y = "Total Trending Videos",
       x = "Views",
       title = "Total Trending Videos by Max Views (under 1 million) while Trending")+
  theme_gray()
#we can see that this group is skewed to the right - most videos tend to have up to 100,000 after which there's a steep decline

#Segmentation by country
ggplot(views_data_below1m)+
  geom_histogram(aes(views, fill = country), binwidth = 10000)+
  facet_grid(~country)+
  theme_gray()+
  labs(y = "Total Trending Videos",
       x = "Views",
       fill = "Country",
       title = "Total Trending Videos by Max Views (under 1 million) while Trending - Segmentation by Country")
#no significant differences between the three countries

```


### **3.2.4. Is a video required to trend for more than a day in order to get the best results?**
```{r}
views_by_video <- raw_data %>% 
  select(video_id, views, country, trending_date) %>%
  group_by(country, video_id, trending_date) %>%
  summarize (views = max(views),
             count = n()) %>%
  group_by(country, video_id) %>%
  summarize(views = max(views)/1000000,
            count = n())

count_by_days_trending <- views_by_video %>%
    group_by(count) %>%
    summarize(total_count = n())

#quick summary of spread
quantile(views_by_video$views)

ggplot(views_by_video, aes(as.factor(count), views)) +
  geom_boxplot(outlier.shape = NA, fill = "dark red", col = "black", size = 0.1)+
  geom_smooth(aes())+
  coord_cartesian(ylim = c(0, 80))+
  labs(y = "Max Views (million)",
       x = "Total Trending Days",
       title = "The median tends to increase as the number of trending days increases")+
  theme_gray()


ggplot(count_by_days_trending, aes(count, total_count))+
  geom_col(fill = "dark red")+
  labs(y = "Number of Videos",
       x = "Total Trending Days",
       title = "The number of samples in each category decreases (much fewer have 30m views than 3m views) which means that the data can be skewed by outliers")

#by country
ggplot(views_by_video, aes(as.factor(count), views)) +
  geom_boxplot(aes(fill = country), outlier.shape = NA, col = "black", size = 0.1)+
  coord_cartesian(ylim = c(0, 100))+
  scale_x_discrete(breaks = seq(0,38,5))+
  labs(y = "Views (million)",
       x = "Total Trending Days",
       title = "Total Vies by amount of Trending Days")+
  theme_gray()+
  facet_wrap(~country)
#same trend, except for the difference the amount of days a video is trending between the three countries that we previously covered
 
#clear variables
rm(views_by_video, views_data, views_data_below1m, views_summary, count_by_days_trending ,max_limit)
```

 
## Additional data fields that could help gain an even better understanding:
1) Public Data: 
* **Subscriber Count** (if made public) as they could be a significant contributor to views (especially soon after a video gets posted)
* other **social media profiles** (high fan base could contribute to the engagement)
* **Share Count** (on Facebook, Instagram, Reddit, etc)
* **Video Length**
* **Search Engine Optimisation**

2) Private Data (only the creator can access these)
* **Watch Time** and **% of Video Watched** (ie. if a video has a total length of 10 mins, what gets watched on average)
* **% Click Through Rate** (ie. out of 100 people that see a video's thumbnail, what % clicks on it)

## Other uses for this dataset:
* **Sentiment** analysis on the comment section
* **Clustering** analysis to analyse different types of videos
* **Regression** analysis to see if we can predict how many views a video will get
* **Statistical** analyses to understand which factors contributed to increased engagement