knitr::kable(pmat,
title = "Matrix of correlation p-values",
digits = 3)
knitr::kable(pmat,
caption = "Matrix of correlation p-values",
digits = 3)
library(ggplot2)
library(dplyr)
library(lubridate)
library(data.table)
library(readr)
library(rjson)
library(jsonlite)
library(ggcorrplot)
library(knitr)
#get datasets for the countries we're interested in (10 countries are available)
gb_data <- read_csv("~/DS/YouTube-Trending-Videos-EDA/Datasets/GBvideos.csv")
us_data <- read_csv("~/DS/YouTube-Trending-Videos-EDA/Datasets/USvideos.csv")
ca_data <- read_csv("~/DS/YouTube-Trending-Videos-EDA/Datasets/CAvideos.csv")
#add a flag so that we know which country belongs to which dataset
gb_data$country <- "GB"
us_data$country <- "US"
ca_data$country <- "CA"
#get category data
us_cat_json <- fromJSON("~/DS/YouTube-Trending-Videos-EDA/Datasets/US_category_id.json")
gb_cat_json <- fromJSON("~/DS/YouTube-Trending-Videos-EDA/Datasets/GB_category_id.json")
ca_cat_json <- fromJSON("~/DS/YouTube-Trending-Videos-EDA/Datasets/CA_category_id.json")
summary(us_cat_json)
#bind together
US_category <-  as.data.frame(cbind(us_cat_json[["items"]][["id"]], us_cat_json[["items"]][["snippet"]][["title"]]))
GB_category <-  as.data.frame(cbind(gb_cat_json[["items"]][["id"]], gb_cat_json[["items"]][["snippet"]][["title"]]))
CA_category <-  as.data.frame(cbind(ca_cat_json[["items"]][["id"]], ca_cat_json[["items"]][["snippet"]][["title"]]))
#change column names
names(US_category) <- c("category_id","category_title")
names(GB_category) <- c("category_id","category_title")
names(CA_category) <- c("category_id","category_title")
#merge data
us_data <- merge(x = us_data, y = US_category, by = "category_id")
gb_data <- merge(x = gb_data, y = GB_category, by = "category_id")
ca_data <- merge(x = ca_data, y = CA_category, by = "category_id")
#combine into one dataset
raw_data <- as.data.table(rbind(gb_data, us_data, ca_data))
#remove unnecessary variables from memory
rm(us_data, gb_data, ca_data)
rm(US_category, GB_category, CA_category)
rm(us_cat_json, gb_cat_json, ca_cat_json)
#I usually like to save a backup of the raw data to make it easier to revert changes (if needed) so let's do that
raw_data_backup <- raw_data
#let's check the structure of the data
glimpse(raw_data)
#we can spot a few class problems as well as add some additional columns
#clean and format dates/times
raw_data$trending_date <- ydm(raw_data$trending_date)
raw_data$publish_date <- ymd(substr(raw_data$publish_time,start = 1,stop = 10))
raw_data$hour <- format(strptime(raw_data$publish_time,"%Y-%m-%d %H:%M:%S"),'%H')
raw_data$days_diff <- as.numeric(raw_data$trending_date-raw_data$publish_date)
#remove unnecessary fields
raw_data <- raw_data %>%
select(-description, -tags, -category_id, -publish_time)
#add new columns for further analysis
raw_data <- raw_data %>%
mutate(perc_engagement = round((likes + dislikes + comment_count) / views, digits = 2)*100,
perc_likes = round(likes / (likes+dislikes), digits=2)*100,
perc_comments = round(comment_count/views, digits = 2) * 100)
#we also want to change hour and country fields to factors
raw_data$hour <- as.factor(raw_data$hour)
raw_data$country <- as.factor(raw_data$country)
#remove video_error_or_removed videos as we do not want these as these videos would've been deleted/errors/copyright violations
table(raw_data$video_error_or_removed)
#it's looks like it's an extremely dataset so we can safely remove these
video_error_or_removed <- raw_data %>%
filter (video_error_or_removed == "TRUE")%>%
select(country, channel_title, title, video_id) %>%
group_by(country, channel_title, title, video_id) %>%
summarize(count = n()) %>% #count = days_trending
arrange(desc(count))
#let's also remove the unecessary column and clean the variable from the environment
raw_data <- raw_data %>%
filter (video_error_or_removed == "FALSE") %>%
select (-video_error_or_removed)
rm(video_error_or_removed)
#check for NAs
raw_data %>% summarise_all(~ sum(is.na(.)))
#720 NA values but at this point we're not sure if that's because of the fact that some creators choose to hide their ratings
raw_data %>% filter(is.na(perc_likes)) %>% group_by(ratings_disabled) %>% summarize (count = n())
#it looks like that's the case for al but 3 of them
#these three videos show as NA due to not having any likes/dislikes so we will adjust the calculation slightly
raw_data <- raw_data %>% mutate(perc_likes = ifelse(ratings_disabled == "FALSE" & likes==0, 0, round(likes / (likes+dislikes), digits=2)*100))
#let's check
raw_data %>% filter(is.na(perc_likes)) %>% group_by(ratings_disabled) %>% summarize (count = n())
#this error has been fixed now and NAs are only showing for those with ratings disabled
#check for NULLs
raw_data %>% summarise_all(~ sum(is.null(.)))
#no issues with NULLs
#check how much of the data has comments disabled
table(quantile(raw_data$comments_disabled, probs = seq(0, 1, length.out=101)))
#check how much of the data has ratings disabled
table(quantile(raw_data$ratings_disabled, probs = seq(0, 1, length.out=101)))
#back up our existing data one last time
raw_data_backup <- raw_data
raw_data_corr <- raw_data %>% select(views,likes,dislikes, comment_count, days_diff)
# Compute a correlation matrix
corr <- round(cor(raw_data_corr), 2)
# Compute a matrix of correlation p-values
pmat <- cor_pmat(raw_data_corr)
knitr::kable(pmat,
caption = "Fig 1 - Matrix of correlation p-values",
digits = 3)
# Visualize the correlation matrix
ggcorrplot(corr, method = "square",
ggtheme = ggplot2::theme_minimal,
title = "Medium to High correlations between our main variables",
outline.col = "black",
colors = c("blue","white", "red"),
lab = TRUE,
digits = 2)
#set up overall dataset
trending_summary <- raw_data %>%
select(country,video_id) %>%
group_by(country) %>%
summarize(count = n(),
unique = n_distinct(video_id))
ggplot(trending_summary)+
geom_col(aes(country, count, fill = country), show.legend = FALSE)+
geom_line(aes(country,unique), group = 1, lwd = 1, col = "black", lty = 2)+
geom_label(aes(country, unique, label = paste(round(unique/count*100), "% unique", sep = "")), fill = "black", col = "white")+
labs(y = "Total Trending Videos",
x = NULL,
title = "Despite a similar number of Total Trending Videos, CA has a different \nprofile to GB and US with considerably more unique videos")
#set up data
trending_duration <- raw_data %>%
select(country, video_id, trending_date) %>%
group_by(country, video_id) %>%
summarize(count = n())
ggplot(trending_duration, aes(count, ..prop.., fill = country))+
geom_bar()+
facet_grid(.~country)+
theme(legend.position = "none") +
labs(y = "% of Unique Trending Videos",
x = "Max Trending Days",
fill = "Country",
title = "CA has a short trending timespan where >80% videos trend 1-2 days \nIn the US and GB this is more spread out: only 10-20% trend for 1-2 days",
subtitle = "It is more common for a video to trend for 20-30 days in the GB/US than 5 days in CA")
#set up overall dataset
trending_summary <- raw_data %>%
select(country,video_id) %>%
group_by(country) %>%
summarize(count = n(),
unique = n_distinct(video_id))
ggplot(trending_summary)+
geom_col(aes(country, count, fill = country), show.legend = FALSE)+
geom_line(aes(country,unique), group = 1, lwd = 1, col = "black", lty = 2)+
geom_label(aes(country, unique, label = paste(round(unique/count*100), "% unique", sep = "")), fill = "black", col = "white")+
labs(y = "Total Trending Videos",
x = NULL,
title = "Despite a similar number of total trending videos, CA has a different \nprofile to GB and US with considerably more unique videos")
#set up data
trending_duration <- raw_data %>%
select(country, video_id, trending_date) %>%
group_by(country, video_id) %>%
summarize(count = n())
ggplot(trending_duration, aes(count, ..prop.., fill = country))+
geom_bar()+
facet_grid(.~country)+
theme(legend.position = "none") +
labs(y = "% of Unique Trending Vide\os",
#set up data
trending_duration <- raw_data %>%
select(country, video_id, trending_date) %>%
group_by(country, video_id) %>%
summarize(count = n())
ggplot(trending_duration, aes(count, ..prop.., fill = country))+
geom_bar()+
facet_grid(.~country)+
theme(legend.position = "none") +
labs(y = "% of Unique Trending Vide\os",
#set up data
trending_duration <- raw_data %>%
select(country, video_id, trending_date) %>%
group_by(country, video_id) %>%
summarize(count = n())
ggplot(trending_duration, aes(count, ..prop.., fill = country))+
geom_bar()+
facet_grid(.~country)+
theme(legend.position = "none") +
labs(y = "% of Unique Trending Videos",
x = "Max Trending Days",
fill = "Country",
title = "CA has a short trending timespan where >80% videos trend 1-2 days \nIn the US and GB this is more spread out: only 10-20% trend for 1-2 days",
subtitle = "It is more common for a video to trend for 20-30 days in the GB/US than 5 days in CA")
max_limit <- quantile(trending_data$days_diff, probs = c(0.99))
#create a new dataset to help answer this question
trending_data <- raw_data %>%
select(country, video_id, views, days_diff) %>%
group_by(country, video_id) %>%
summarize(count = n_distinct(video_id),
views = max(views),
days_diff = min(days_diff))
max_limit <- quantile(trending_data$days_diff, probs = c(0.99))
#let's visualise this
ggplot(trending_data, aes(as.factor(days_diff), count))+
geom_col(fill = "dark red")+
coord_cartesian(xlim=c(0,max_limit))+
labs(y = "Unique Trending Videos",
x = "Days Until Trending",
title = "~75% of videos became trending within their first day and ~95% took < 5 days",
subtitle = "It is unlikely for a video to reach the trending page if it hasn't done so within the first 10 days")
#Is there a difference between countries?
ggplot(trending_data, aes(days_diff, count, fill = country))+
geom_col()+
coord_cartesian(xlim=c(0,max_limit))+ #zooming in on the area of interest (99% of the data)
facet_grid(.~country)+
labs(y = "Unique Trending Videos",
x = "Days Until Trending",
fill = "Country",
title = "Videos in Canada videos tend to become trending very quickly, \nunlike the US where this spread is wider, and GB where it is widest")
#Is there a difference between countries?
ggplot(trending_data, aes(days_diff, count, fill = country))+
geom_col()+
coord_cartesian(xlim=c(0,max_limit))+ #zooming in on the area of interest (99% of the data)
facet_grid(.~country)+
theme(legend.position = "none") +
labs(y = "Unique Trending Videos",
x = "Days Until Trending",
fill = "Country",
title = "Videos in Canada videos tend to become trending very quickly, \nunlike the US where this spread is wider, and GB where it is widest")
#create dataset
views_summary <- raw_data %>%
select(country,video_id,views) %>%
group_by(country, video_id) %>%
summarize(count = n(),
unique = n_distinct(video_id),
views = max(views)) %>%
group_by(country) %>%
summarize(count = sum(count),
unique = round(sum(unique)/1000),
views = sum(views)/1000000000)
ggplot(views_summary)+
geom_col(aes(country, views, fill = country))+
geom_line(aes(country,unique), group = 1, lwd = 2, col = "black", lty = 3)+
geom_label(aes(country, unique, label = unique), fill = "black", col = "white")+
theme(legend.position = "none") +
annotate("text", x = 2.7, y = 7.35, label = "Unique Trending Videos (thousands)", angle = 14, lwd = 4.5)+
labs(y = "Total Views (billions)",
x = "Country",
title = "CA leads in total views (8x more unique videos than GB and 4x more than US) \nGB has more Views than US, despite having ~50% fewer videos")
#create dataset
views_summary <- raw_data %>%
select(country,video_id,views) %>%
group_by(country, video_id) %>%
summarize(count = n(),
unique = n_distinct(video_id),
views = max(views)) %>%
group_by(country) %>%
summarize(count = sum(count),
unique = round(sum(unique)/1000),
views = sum(views)/1000000000)
ggplot(views_summary)+
geom_col(aes(country, views, fill = country))+
geom_line(aes(country,unique), group = 1, lwd = 2, col = "black", lty = 3)+
geom_label(aes(country, unique, label = unique), fill = "black", col = "white")+
theme(legend.position = "none") +
annotate("text", x = 2.7, y = 7.35, label = "Unique Trending Videos (thousands)", angle = 14, lwd = 4.5)+
labs(y = "Total Views (billions)",
x = NULL,
title = "CA leads in total views (8x more unique videos than GB and 4x more than US) \nGB has more Views than US, despite having ~50% fewer videos")
max_limit <- quantile(views_data$views_m_reached, probs = c(0.99))
views_data <- raw_data %>%
select(country,video_id,views) %>%
group_by(country, video_id) %>%
summarize(count = n(),
unique = n_distinct(video_id),
views = max(views),
views_m_reached = floor(views/1000000),
views_rounded = round(views/1000000))
max_limit <- quantile(views_data$views_m_reached, probs = c(0.99))
ggplot(views_data, aes(views_m_reached, count))+
geom_col(fill="dark red")+
coord_cartesian(xlim=c(0,max_limit))+
labs(y = "Unique Trending Videos",
x = "Views (millions)",
title = "Only ~25% of videos receive more than 1 million Views \nMost (~95%) of the videos haven't reached the 5m View Count")+
geom_vline(xintercept=0.51, lwd = 1, col = "black", lty = 1)+
annotate("text", x = 0.8, y = 29000, label = "76% of data", angle = 90)+
geom_vline(xintercept=4.5, lwd = 1, col = "black", lty = 5)+
annotate("text", x = 4.8, y = 29000, label = "95% of data", angle = 90)+
geom_vline(xintercept=max_limit+0.5, lwd = 1, col = "black", lty = 2)+
annotate("text", x = max_limit-0.3+0.4, y = 29000, label = "99% of data", angle = 90)
max_limit <- quantile(views_data$views_m_reached, probs = c(0.99))
ggplot(views_data, aes(views_m_reached, count))+
geom_col(fill="dark red")+
coord_cartesian(xlim=c(0,max_limit))+
labs(y = "Unique Trending Videos",
x = "Views (millions)",
title = "Only ~25% of videos receive more than 1 million views \nMost (~95%) of the videos haven't reached the 5m view count")+
geom_vline(xintercept=0.51, lwd = 1, col = "black", lty = 1)+
annotate("text", x = 0.8, y = 29000, label = "76% of data", angle = 90)+
geom_vline(xintercept=4.5, lwd = 1, col = "black", lty = 5)+
annotate("text", x = 4.8, y = 29000, label = "95% of data", angle = 90)+
geom_vline(xintercept=max_limit+0.5, lwd = 1, col = "black", lty = 2)+
annotate("text", x = max_limit-0.3+0.4, y = 29000, label = "99% of data", angle = 90)
views_by_video <- raw_data %>%
select(video_id, views, country, trending_date) %>%
group_by(country, video_id, trending_date) %>%
summarize (views = max(views),
count = n()) %>%
group_by(country, video_id) %>%
summarize(views = max(views)/1000000,
count = n())
ggplot(views_by_video, aes(as.factor(count), views)) +
geom_boxplot(outlier.shape = NA, fill = "dark red", col = "black", size = 0.1)+
geom_smooth(aes())+
coord_cartesian(ylim = c(0, 80))+
labs(y = "Max views (million)",
x = "Total Trending Days",
title = "The longer a video trends, the more views it accumulates (excludes outliers)",
subtitle = "However, the sample size decreases significantly with each increase in Total Trending Days")
ggplot(raw_data_cat) +
geom_bar(aes(reorder(category_title, views), views, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title,views), unique_count), group = 1, lwd = 0.5, lty = 1) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "Number of Views (millions)",
x = NULL,
title = "In the GB, there is a significantly higher number of views within Music \nthan Entertainment, despite a comparable unique video count",
subtitle = "Bar chart - Total Video Count \nLine chart - Unique Video Count")+
coord_flip()
raw_data_cat <- raw_data %>%
select(country, category_title, video_id, views) %>%
group_by(country, category_title, video_id) %>%
summarize (count= n(),
unique_count = n_distinct(video_id),
views = max(views)) %>%
group_by(country, category_title) %>%
summarize (count = sum(count),
unique_count = sum(unique_count),
views = sum(views)/1000000) %>%
arrange(country,desc(count))
#let's visualise (line indicates unique count)
ggplot(raw_data_cat, aes(x=reorder(category_title, count), y = count, fill = category_title)) +
geom_bar(stat = "identity") +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "Unique Videos",
x = NULL,
title = "Entertainment in the clear outlier in CA\nAlongside Entertainment, Music is very common in GB and US")+
coord_flip()
ggplot(raw_data_cat) +
geom_bar(aes(reorder(category_title, views), views, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title,views), unique_count), group = 1, lwd = 0.5, lty = 1) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "Number of Views (millions)",
x = NULL,
title = "In the GB, there is a significantly higher number of views within Music \nthan Entertainment, despite a comparable unique video count",
subtitle = "Bar chart - Total Video Count \nLine chart - Unique Video Count")+
coord_flip()
ggplot(raw_data_cat) +
geom_bar(aes(reorder(category_title, views), views, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title,views), unique_count), group = 1, lwd = 0.5, lty = 1) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "Number of Views (millions)",
x = NULL,
title = "In GB, there is a significantly higher number of views within Music \nthan Entertainment, despite a comparable unique video count",
subtitle = "Bar chart - Total Video Count \nLine chart - Unique Video Count")+
coord_flip()
raw_data_cat_detail <- raw_data %>%
select(country, category_title, video_id, views) %>%
group_by(country, category_title, video_id) %>%
summarize (count= n(),
unique_count = n_distinct(video_id),
views = max(views)/1000000) %>%
arrange(country,desc(count))
#excluding outliers
ggplot(raw_data_cat_detail, aes(reorder(category_title, count), views, fill = category_title)) +
geom_boxplot(outlier.shape = NA) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "Number of Views (millions)",
x = NULL,
title = "Irrespective of country, Music has the largest spread of views",
subtitle = "Excludes outliers")+
coord_flip(ylim=c(0,25))
raw_data_eng_overall <- raw_data %>%
select(country, category_title, video_id, trending_date, likes, comment_count, dislikes, views) %>%
group_by (country) %>%
summarize (perc_engagement = ((sum(likes)+sum(dislikes)+sum(comment_count))/sum(views))*100,
perc_dislikes = (sum(dislikes)/(sum(likes)+sum(dislikes)))*100,
perc_comments_to_views = (sum(comment_count)/sum(views))*100)
ggplot(raw_data_eng_overall) +
geom_col(aes(country, perc_dislikes, fill = country)) +
geom_line(aes(country, perc_engagement), group = 1, lwd = 1, lty = 3) +
theme(legend.position = "none") +
labs(y = "% dislikes / % engagement",
x = "Country",
title = "GB has a higher % of dislikes and lower overall engagement",
subtitle = "Bar chart - % of Dislikes ( Dislikes / ( Likes + Dislikes ) ); \nDotted Line chart - % engagement ( ( Likes + Dislikes + Comments ) / Views )")
raw_data_eng_overall <- raw_data %>%
select(country, category_title, video_id, trending_date, likes, comment_count, dislikes, views) %>%
group_by (country) %>%
summarize (perc_engagement = ((sum(likes)+sum(dislikes)+sum(comment_count))/sum(views))*100,
perc_dislikes = (sum(dislikes)/(sum(likes)+sum(dislikes)))*100,
perc_comments_to_views = (sum(comment_count)/sum(views))*100)
ggplot(raw_data_eng_overall) +
geom_col(aes(country, perc_dislikes, fill = country)) +
geom_line(aes(country, perc_engagement), group = 1, lwd = 1, lty = 3) +
theme(legend.position = "none") +
labs(y = "% dislikes / % engagement",
x = NULL,
title = "GB has a higher % of dislikes and lower overall engagement",
subtitle = "Bar chart - % of Dislikes ( Dislikes / ( Likes + Dislikes ) ); \nDotted Line chart - % engagement ( ( Likes + Dislikes + Comments ) / Views )")
#let's create a dataset at category level that we can use for further analysis
raw_data_eng <- raw_data %>%
select(country, category_title, video_id, trending_date, likes, comment_count, dislikes, views) %>%
group_by(country, category_title, video_id) %>%
summarize (trending_date = max(trending_date),
likes = max(likes),
comments = max(comment_count),
dislikes = max(dislikes),
views = max(views),
count= n(),
unique_count = n_distinct(video_id))%>%
group_by(country, category_title) %>%
summarize (perc_engagement = round((sum(likes)+sum(dislikes)+sum(comments))/sum(views)*100),
perc_likes = round((sum(likes)/(sum(likes)+sum(dislikes)))*100),
perc_dislikes = round((sum(dislikes)/(sum(likes)+sum(dislikes)))*100),
perc_comments = round((sum(comments)/sum(views))*100),
likes = mean(likes)/1000,
views = sum(views)/1000000000)%>%
filter(!category_title %in% c("Shows", "Nonprofits & Activism", "Movies"))
ggplot(raw_data_eng) +
geom_bar(aes(reorder(category_title, perc_dislikes), perc_dislikes, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title, perc_dislikes), views), group = 1, lwd = 1, lty = 3) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "% dislikes",
x = NULL,
title = "The most disliked categories are News&Politics and Entertainment \nThe most liked categories were Comedy and Pets&Animals",
subtitle = "Dotted line indicates unique videos across each category")+
coord_flip()
#let's create a dataset at category level that we can use for further analysis
raw_data_eng <- raw_data %>%
select(country, category_title, video_id, trending_date, likes, comment_count, dislikes, views) %>%
group_by(country, category_title, video_id) %>%
summarize (trending_date = max(trending_date),
likes = max(likes),
comments = max(comment_count),
dislikes = max(dislikes),
views = max(views),
count= n(),
unique_count = n_distinct(video_id))%>%
group_by(country, category_title) %>%
summarize (perc_engagement = round((sum(likes)+sum(dislikes)+sum(comments))/sum(views)*100),
perc_likes = round((sum(likes)/(sum(likes)+sum(dislikes)))*100),
perc_dislikes = round((sum(dislikes)/(sum(likes)+sum(dislikes)))*100),
perc_comments = round((sum(comments)/sum(views))*100),
likes = mean(likes)/1000,
views = sum(views)/1000000000)%>%
filter(!category_title %in% c("Shows", "Nonprofits & Activism", "Movies"))
ggplot(raw_data_eng) +
geom_bar(aes(reorder(category_title, perc_dislikes), perc_dislikes, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title, perc_dislikes), views), group = 1, lwd = 1, lty = 3) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "% dislikes",
x = NULL,
title = "The most disliked categories were News&Politics and Entertainment \nThe most liked categories were Comedy and Pets&Animals",
subtitle = "Dotted line indicates unique videos across each category")+
coord_flip()
#let's create a dataset at category level that we can use for further analysis
raw_data_eng <- raw_data %>%
select(country, category_title, video_id, trending_date, likes, comment_count, dislikes, views) %>%
group_by(country, category_title, video_id) %>%
summarize (trending_date = max(trending_date),
likes = max(likes),
comments = max(comment_count),
dislikes = max(dislikes),
views = max(views),
count= n(),
unique_count = n_distinct(video_id))%>%
group_by(country, category_title) %>%
summarize (perc_engagement = round((sum(likes)+sum(dislikes)+sum(comments))/sum(views)*100),
perc_likes = round((sum(likes)/(sum(likes)+sum(dislikes)))*100),
perc_dislikes = round((sum(dislikes)/(sum(likes)+sum(dislikes)))*100),
perc_comments = round((sum(comments)/sum(views))*100),
likes = mean(likes)/1000,
views = sum(views)/1000000000)%>%
filter(!category_title %in% c("Shows", "Nonprofits & Activism", "Movies"))
ggplot(raw_data_eng) +
geom_bar(aes(reorder(category_title, perc_dislikes), perc_dislikes, fill = category_title), stat = "identity") +
geom_line(aes(reorder(category_title, perc_dislikes), views), group = 1, lwd = 1, lty = 3) +
facet_grid(~country) +
theme(legend.position = "none") +
labs(y = "% dislikes",
x = NULL,
title = "The most disliked categories: News&Politics and Entertainment \nThe most liked categories: Comedy and Pets&Animals",
subtitle = "Dotted line indicates unique videos across each category")+
coord_flip()
